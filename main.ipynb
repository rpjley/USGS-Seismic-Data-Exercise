{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise SEP Developer Test**"
      ],
      "metadata": {
        "id": "TJe6BmxN3Plw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is designed to assess your ability to design and develop a project from a limited set of goals, provided below. We will evaluate your project based on its fulfillment of the outlined objectives, code efficiency, readability, and adherence to best practices. Additionally, we will evaluate the structure and content of your GitHub repository according to standard practices.\n",
        "\n",
        "1. Please perform all coding within the Google Colab environment: http://colab.research.google.com\n",
        "    * Please store your project in a GitHub repository, so we can fork and run it in Colab.\n",
        "        * Additionally, please create a standard GitHub repository with the expected files and directory structure.    \n",
        "    * Please note that Colab doesn't automatically include additional files. To avoid potential issues, we recommend only including files that the evaluators already have access to.    \n",
        "    * The evaluators will use the files SEP01.mseed, SEP02.mseed, and SEP03.mseed, which can be found in this GitHub project.\n",
        "2. Import the mseed files\n",
        "3. Create a database with proper normal form and constraints\n",
        "4. Import the data into the newly created database\n",
        "5. Create a visualization for the data. The visualization can be static but must include:    \n",
        "    * A title\n",
        "    * Text section displaying metadata about the miniseed data\n",
        "    * Helicorder-style charts of the miniseed data (These may be built from the database data or the mseed files)\n",
        "    * A map with icons indicating station locations based on the miniseed data (station lat and lon can be found on iris' API)\n",
        "    * Add to the map additional stations found on IRIS' data API. Stations of interest include HOA and SUG\n",
        "        * Data API information can be found at http://service.iris.edu/fdsnws/station/1/\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "a5f77d4a004bc4ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install Packages**\n"
      ],
      "metadata": {
        "id": "LoRlM_-IEXnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "-56ONW4NsHo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages used in this project include:\n",
        "- **ObsPy** - used for seismic data processing and analysis.\n",
        "- **Folium** - used for creating interactive maps of seismic station locations.\n",
        "- **SQLAlchemy** (installed via ObsPy) - supports database operations for managing seismic station and waveform data."
      ],
      "metadata": {
        "id": "_Y8Zu14Tr12X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "0GiT6OIVsBCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary quietly\n",
        "!pip install -q obspy folium > /dev/null 2>&1\n",
        "\n",
        "# Confirm successful installation\n",
        "print(\"All Packages Successfully Installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFAgoJexEbML",
        "outputId": "b0cc1174-3654-4884-80db-a47a1385b364"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Packages Successfully Installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Database Creation and Testing**\n"
      ],
      "metadata": {
        "id": "mnwTHcnMG7lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "YKJmntwsuuyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This database is implemented using SQLAlchemy, enabling integration with Python objects, and is set up using file-based stucture named seismic_data.db.\n",
        "\n",
        "\n",
        "####Database Structure\n",
        "1. **Station Table**:\n",
        "   - Stores metadata about seismic stations.\n",
        "   - **Columns**:\n",
        "     - `station_id`: Unique identifier for each station (Primary Key).\n",
        "     - `station`: Name of the station.\n",
        "     - `network`: Network code.\n",
        "     - `channel`: Channel code (e.g., EHZ).\n",
        "     - `latitude`: Latitude of the station.\n",
        "     - `longitude`: Longitude of the station.\n",
        "\n",
        "2. **WaveformTrace Table**:\n",
        "   - Stores individual seismic waveform traces.\n",
        "   - **Columns**:\n",
        "     - `trace_id`: Unique identifier for each trace (Primary Key).\n",
        "     - `station_id`: Links to the corresponding station (Foreign Key).\n",
        "     - `start_time`: Start time of the trace.\n",
        "     - `end_time`: End time of the trace.\n",
        "     - `trace_data`: Serialized binary data of the waveform.\n",
        "     - `sampling_rate`: Sampling rate of the waveform in Hz.\n",
        "\n",
        "####Functions in the Database Creation\n",
        "\n",
        "1. **setup_database()**:\n",
        "   - Initializes the database.\n",
        "   - Creates tables for `Station` and `WaveformTrace`.\n",
        "   - Returns an file-based SQLite engine and a session object.\n",
        "\n",
        "2. **initialize_database(stream, session)**:\n",
        "   - Populates the database with waveform data from an ObsPy `Stream` object.\n",
        "   - For each trace:\n",
        "     - Adds station metadata if it doesn’t already exist.\n",
        "     - Adds the individual seismic waveform traces.\n",
        "\n",
        "3. **get_coordinates(station_name)**:\n",
        "   - Uses IRIS data API to pull lattitude and longitude if they are missing from the .mseed data\n",
        "   -If IRIS data API is missing, allows user to manually record the coordinates.\n",
        "\n",
        "####Functions in the Unit Tests\n",
        "\n",
        "\n",
        "1. **test_full_trace_count**:\n",
        "   - Validates that all traces from the stream are correctly stored in the database.\n",
        "   - Ensures the total count matches the expected number of traces (32).\n",
        "\n",
        "2. **test_query_by_time_range**:\n",
        "   - Checks the database's ability to filter traces by a specific time range.\n",
        "   - Confirms the expected number of traces fall within the queried time window.\n",
        "\n",
        "3. **test_query_by_station**:\n",
        "   - Tests filtering traces by the station name.\n",
        "   - Ensures all returned traces correspond to the station \"SEP\".\n",
        "\n",
        "4. **test_query_by_sampling_rate**:\n",
        "   - Verifies filtering by the sampling rate.\n",
        "   - Confirms all traces have the specified sampling rate.\n",
        "\n",
        "5. **test_query_by_channel**:\n",
        "   - Ensures the database can filter traces by channel.\n",
        "   - Validates that all results belong to the queried channel.\n",
        "\n",
        "6. **test_query_by_location**:\n",
        "   - Confirms traces can be filtered by geographic coordinates (latitude and longitude).\n",
        "   - Verifies all returned traces correspond to the specified location.\n",
        "\n",
        "7. **test_validate_trace_metadata**:\n",
        "   - Validates metadata and waveform data for individual traces.\n",
        "   - Compares stored values (e.g., start time, end time, sampling rate, data length) with expected results.\n",
        "   - Ensures data integrity in the database."
      ],
      "metadata": {
        "id": "695MKgChuynC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "LWZVDptShpkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from obspy import Trace, Stream, UTCDateTime, read\n",
        "from obspy.clients.fdsn import Client\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Float, LargeBinary, ForeignKey\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "import pickle\n",
        "\n",
        "#This database uses the version of SQLAlchemy compatible with Obspy as opposed to the newest version\n",
        "os.environ[\"SQLALCHEMY_SILENCE_UBER_WARNING\"] = \"1\"\n",
        "\n",
        "#Environment Setup\n",
        "Base = declarative_base()\n",
        "client = Client(\"IRIS\")\n",
        "\n",
        "class Station(Base):\n",
        "    __tablename__ = 'stations'\n",
        "    station_id = Column(Integer, primary_key=True, autoincrement=True)\n",
        "    station = Column(String, nullable=False)\n",
        "    network = Column(String, nullable=False)\n",
        "    channel = Column(String, nullable=False)\n",
        "    latitude = Column(Float, nullable=True)\n",
        "    longitude = Column(Float, nullable=True)\n",
        "\n",
        "class WaveformTrace(Base):\n",
        "    __tablename__ = 'waveform_traces'\n",
        "    trace_id = Column(Integer, primary_key=True, autoincrement=True)\n",
        "    station_id = Column(Integer, ForeignKey('stations.station_id'), nullable=False)\n",
        "    start_time = Column(String, nullable=False)\n",
        "    end_time = Column(String, nullable=False)\n",
        "    trace_data = Column(LargeBinary, nullable=False)\n",
        "    sampling_rate = Column(Float, nullable=False)\n",
        "\n",
        "def setup_database():\n",
        "    #Creates database named \"sqlite:///seismic_data.db\"\n",
        "    #create_engine(\"sqlite:///:memory:\") could be used for faster processing but less scalability\n",
        "    engine = create_engine(\"sqlite:///seismic_data.db\")\n",
        "    Base.metadata.create_all(engine)\n",
        "    Session = sessionmaker(bind=engine)\n",
        "    session = Session()\n",
        "    return engine, session\n",
        "\n",
        "def get_coordinates(station_name):\n",
        "    #Pulls coordinates from IRIS API using station name or user input if unavailable\n",
        "    latitude = None\n",
        "    longitude = None\n",
        "    try:\n",
        "        inventory = client.get_stations(station=station_name, level=\"station\")\n",
        "        station_info = inventory[0][0]\n",
        "        latitude = station_info.latitude\n",
        "        longitude = station_info.longitude\n",
        "    except:\n",
        "        print(f\"Unable to retrieve coordinates automatically:\")\n",
        "        while latitude is None:\n",
        "            try:\n",
        "                latitude = float(input(\"Enter latitude: \"))\n",
        "            except ValueError:\n",
        "              print(\"Invalid input. Please enter a numeric value for latitude.\")\n",
        "            while longitude is None:\n",
        "              try:\n",
        "                longitude = float(input(\"Enter longitude: \"))\n",
        "              except ValueError:\n",
        "                print(\"Invalid input. Please enter a numeric value for longitude.\")\n",
        "    return latitude, longitude\n",
        "\n",
        "def initialize_database(stream, session):\n",
        "    #Initialize database with stream and session\n",
        "    for trace in stream:\n",
        "        station_entry = session.query(Station).filter_by(\n",
        "            station=trace.stats.station, network=trace.stats.network\n",
        "        ).first()\n",
        "        #.mseed data may not have location\n",
        "        if not station_entry:\n",
        "            latitude = None\n",
        "            longitude = None\n",
        "\n",
        "            if hasattr(trace.stats, 'latitude') and hasattr(trace.stats, 'longitude'):\n",
        "                latitude = trace.stats.latitude\n",
        "                longitude = trace.stats.longitude\n",
        "            else:\n",
        "                #Search IRIS for station location\n",
        "                latitude, longitude = get_coordinates(trace.stats.station)\n",
        "\n",
        "            station_entry = Station(\n",
        "                station=trace.stats.station,\n",
        "                network=trace.stats.network,\n",
        "                channel=trace.stats.channel,\n",
        "                latitude=latitude,\n",
        "                longitude=longitude\n",
        "            )\n",
        "            session.add(station_entry)\n",
        "            session.commit()\n",
        "\n",
        "        #Add each trace to the database\n",
        "        new_trace = WaveformTrace(\n",
        "            station_id=station_entry.station_id,\n",
        "            start_time=str(trace.stats.starttime),\n",
        "            end_time=str(trace.stats.endtime),\n",
        "            trace_data=pickle.dumps(trace.data),\n",
        "            sampling_rate=trace.stats.sampling_rate\n",
        "        )\n",
        "        session.add(new_trace)\n",
        "\n",
        "    session.commit()\n",
        "    print(\"Database initialized successfully.\")"
      ],
      "metadata": {
        "id": "9b1GDfOTHEev"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unit Tests"
      ],
      "metadata": {
        "id": "4Hnt0icuHBgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from obspy import Stream, read\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy import create_engine\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def setup_database():\n",
        "    #Database setup from the previous code for unit tests\n",
        "    engine = create_engine(\"sqlite:///seismic_data.db\")\n",
        "    Base.metadata.create_all(engine)\n",
        "    Session = sessionmaker(bind=engine)\n",
        "    session = Session()\n",
        "    return engine, session\n",
        "\n",
        "class VerboseTestResult(unittest.TextTestResult):\n",
        "    #Add custom success and failure messages\n",
        "    def addSuccess(self, test):\n",
        "        super().addSuccess(test)\n",
        "        self.stream.writeln(f\"✔️ SUCCESS: {test.shortDescription() or str(test)}\")\n",
        "\n",
        "    def addFailure(self, test, err):\n",
        "        super().addFailure(test, err)\n",
        "        self.stream.writeln(f\"❌ FAILURE: {test.shortDescription() or str(test)}\")\n",
        "\n",
        "class VerboseTestRunner(unittest.TextTestRunner):\n",
        "    def _makeResult(self):\n",
        "        return VerboseTestResult(self.stream, self.descriptions, self.verbosity)\n",
        "\n",
        "class TestDatabase(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        if os.path.exists(\"seismic_data.db\"):\n",
        "          os.remove(\"seismic_data.db\")\n",
        "        #Setup database and session for each test\n",
        "        self.engine, self.session = setup_database()\n",
        "\n",
        "        #Load traces directly from .mseed files\n",
        "        self.stream = Stream()\n",
        "        for file in [\"SEP/SEP01.mseed\", \"SEP/SEP02.mseed\", \"SEP/SEP03.mseed\"]:\n",
        "            try:\n",
        "                self.stream += read(file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file}: {e}\")\n",
        "\n",
        "        #Initialize database with these traces\n",
        "        initialize_database(self.stream, self.session)\n",
        "\n",
        "    def test_full_trace_count(self):\n",
        "        \"Query by Full Trace Count\"\n",
        "        #Query database for all traces\n",
        "        traces = self.session.query(WaveformTrace).all()\n",
        "\n",
        "        #Verify all traces are in the database\n",
        "        self.assertEqual(len(traces), 32, \"All traces should be in the database\")\n",
        "\n",
        "    def test_query_by_time_range(self):\n",
        "        \"Query by Time Range\"\n",
        "        #Query based on a specific time range\n",
        "        start_time = \"2023-11-11T00:00:00.000001Z\"\n",
        "        end_time = \"2023-11-11T02:31:38.090001Z\"\n",
        "\n",
        "        traces = self.session.query(WaveformTrace).filter(\n",
        "            WaveformTrace.start_time >= start_time,\n",
        "            WaveformTrace.end_time <= end_time\n",
        "        ).all()\n",
        "\n",
        "        #Expected number of traces in this range\n",
        "        expected_count = 7\n",
        "        self.assertEqual(len(traces), expected_count, f\"Expected {expected_count} traces in the given time range.\")\n",
        "\n",
        "    def test_query_by_station(self):\n",
        "        \"Query by Station\"\n",
        "        #Query based on station name\n",
        "        station_name = \"SEP\"\n",
        "        traces = self.session.query(WaveformTrace).join(Station).filter(Station.station == station_name).all()\n",
        "\n",
        "        #Verify all traces belong to the station \"SEP\"\n",
        "        self.assertEqual(len(traces), 32, \"All 32 traces should belong to station 'SEP'.\")\n",
        "\n",
        "    def test_query_by_sampling_rate(self):\n",
        "        \"Query by Sampling Rate\"\n",
        "        #Query based on a specific sampling rate\n",
        "        sampling_rate = 100.0\n",
        "        traces = self.session.query(WaveformTrace).filter(WaveformTrace.sampling_rate == sampling_rate).all()\n",
        "\n",
        "        #Verify all traces have a sampling rate of 100.0 Hz\n",
        "        self.assertEqual(len(traces), 32, \"All 32 traces should have a sampling rate of 100.0 Hz.\")\n",
        "\n",
        "    def test_query_by_channel(self):\n",
        "        \"Query by Channel\"\n",
        "        #Query based on a specific channel\n",
        "        channel = \"EHZ\"\n",
        "        traces = self.session.query(WaveformTrace).join(Station).filter(Station.channel == channel).all()\n",
        "\n",
        "        #Verify all traces belong to the channel \"EHZ\"\n",
        "        self.assertEqual(len(traces), 32, \"All 32 traces should belong to channel 'EHZ'.\")\n",
        "\n",
        "    def test_query_by_location(self):\n",
        "        \"Query by Location\"\n",
        "        #Query based on station latitude and longitude\n",
        "        latitude = 46.19978\n",
        "        longitude = -122.190857\n",
        "        traces = self.session.query(WaveformTrace).join(Station).filter(\n",
        "            Station.latitude == latitude,\n",
        "            Station.longitude == longitude\n",
        "        ).all()\n",
        "\n",
        "        #Verify all traces have the correct coordinates\n",
        "        self.assertEqual(len(traces), 32, \"All traces should belong to the specified location.\")\n",
        "\n",
        "    def test_validate_trace_metadata(self):\n",
        "        \"Validation of Trace Metadata\"\n",
        "        #Query all traces\n",
        "        traces = self.session.query(WaveformTrace).all()\n",
        "\n",
        "        #Verify metadata for the first 7 traces\n",
        "        expected_values = [\n",
        "            (\"2023-11-11T00:00:00.000001Z\", \"2023-11-11T00:36:58.090001Z\", 100.0, 221810),\n",
        "            (\"2023-11-11T00:37:45.100001Z\", \"2023-11-11T01:41:06.090001Z\", 100.0, 380100),\n",
        "            (\"2023-11-11T01:43:07.100001Z\", \"2023-11-11T02:01:05.090001Z\", 100.0, 107800),\n",
        "            (\"2023-11-11T02:01:47.100001Z\", \"2023-11-11T02:02:10.090001Z\", 100.0, 2300),\n",
        "            (\"2023-11-11T02:02:24.100001Z\", \"2023-11-11T02:17:01.090001Z\", 100.0, 87700),\n",
        "            (\"2023-11-11T02:17:49.100001Z\", \"2023-11-11T02:31:16.090001Z\", 100.0, 80700),\n",
        "            (\"2023-11-11T02:31:37.100001Z\", \"2023-11-11T02:31:38.090001Z\", 100.0, 100)\n",
        "        ]\n",
        "\n",
        "        for trace, expected in zip(traces[:7], expected_values):\n",
        "            self.assertEqual(trace.start_time, expected[0], \"Start time does not match\")\n",
        "            self.assertEqual(trace.end_time, expected[1], \"End time does not match\")\n",
        "            self.assertEqual(trace.sampling_rate, expected[2], \"Sampling rate does not match\")\n",
        "            self.assertEqual(len(pickle.loads(trace.trace_data)), expected[3], \"Data length does not match\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(testRunner=VerboseTestRunner, argv=[''], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGNXS0RdQg-c",
        "outputId": "c8fdc71e-4be5-45ed-fc85-ed34a66f001c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Full Trace Count\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Channel\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Location\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Sampling Rate\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Station\n",
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "✔️ SUCCESS: Query by Time Range\n",
            ".✔️ SUCCESS: Validation of Trace Metadata\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 7 tests in 9.135s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database initialized successfully.\n"
          ]
        }
      ]
    }
  ]
}